---
title: "mclust"
author: "Abdou NIANG"
date: "2024-01-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(factoextra)
library(MASS)
library(ggplot2)
library(rio)
setwd = ('~/Dataset')
mydata = import('https://raw.githubusercontent.com/mariocastro73/ML2020-2021/master/datasets/comparing-dends.csv')
head(mydata)
```

1er travail de clustering avec Mclust : On a supposÃ© ici que les donnÃ©es provenaient d'un mÃ©lange de gaussiennes.

Ceci est pûrement dans le but de comparer Rmixmod et Mclust ( voir temps de calcul, proportions etc...)


### Amusons nous d'abord Ã  trouver le nombre optimal de clusters avec la mÃ©thode Kmeans

 Ci-dessous, une petite fonction sympa de R, pour rechercher le nombre optimal de cluster (rÃ¨gle du coud)
 
```{r}
wssplot <- function(data, nc=15, seed=1234){
                  wss <- (nrow(data)-1)*sum(apply(data,2,var))
                      for (i in 2:nc){
                set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
              plot(1:nc, wss, type="b", xlab="Number of Clusters",
                            ylab="Within groups sum of squares")
              wss
       }


```

```{r}
wssplot(mydata)
```

A priori deux clusters suffisent, poursuivons :


```{r}
KM = kmeans(mydata,3)
```





```{r}
library(ggfortify)
autoplot(KM,mydata,frame = TRUE)
```

Les deux clusters sont trÃ¨s bien sÃ©parÃ©s et Kmeans marche trÃ¨s bien.

# Poursuivons cette fois avec les modÃ¨les de mÃ©lange gaussiens en comparant Mclust et Rmixmod

```{r}
#install.packages('mclust')
library(mclust)

```

```{r}
ggplot(mydata,aes(x=x,y=y)) +geom_point() + geom_density2d()
```


Attention ici il ne s'agit pas de gaussiennes ! C'est des densitÃ©s empiriques. Rien Ã  voir avec des densitÃ©s de gaussiennes.








```{r}
library(mclust)
dens <- densityMclust(mydata)
plot(dens,what = 'density',type='persp')
```

Ici on  a bien à  faire des densitÃ©s de gaussiennes.

2Ã¨me chose : On peut bien remarquer qu'il y'a trois gaussiennes chacune avec des paramÃ¨tres diffÃ©rents.

Contrairement à la rÃ¨gle du coude qui souvent est imprÃ©cise, Mclust indique trois gaussiennes bien sÃ©parÃ©es indiquant 3 gaussiennes.



```{r}

mc=Mclust(mydata)

```





```{r}
#plot(mc)
```


```{r}
library(Rmixmod)


RM = mixmodCluster(mydata,3)

RM

```



```{r}
summary(RM)
```


```{r}
hist(RM)
```


```{r}
plot(RM)
```



```{r}
iris
head(iris)
```



```{r}
remaining.obs <- sample(1:nrow(iris), 10)

learn <- mixmodLearn(iris[-remaining.obs, 1:4], iris$Species[-remaining.obs])

prediction <- mixmodPredict(
  data = iris[remaining.obs, 1:4],
  classificationRule = learn["bestResult"]
)

prediction
# compare prediction with real results
paste("accuracy = ", mean(as.integer(iris$Species[remaining.obs]) == prediction["partition"]) * 100,
      "%",
      sep = ""
)
```

```{r}
mixmodGaussianModel()
```
#-----------------------------------#



test sur un jeu de données  :



```{r}
library(dplyr)


n_rows <- 5000
n_cols <- 10


data <- matrix(runif(n_rows * n_cols), nrow = n_rows, ncol = n_cols)


df <- as.data.frame(data)
colnames(df) <- paste0("Feature_", 1:n_cols)

# Affichage des premiÃ¨res lignes du DataFrame
head(df)

```

```{r}

```

```{r}
RM_alea = mixmodCluster(df,3)
RM_alea
```

```{r}

plotCluster(RM_alea["bestResult"],df)
```






